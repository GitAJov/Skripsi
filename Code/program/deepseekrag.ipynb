{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842ededb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "model = OllamaLLM(model='deepseekmini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b42a268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# class ContrieverEmbeddings(Embeddings):\n",
    "#     def __init__(self, model_name=\"facebook/contriever-msmarco\", device=\"cpu\"):\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "#         self.device = device\n",
    "\n",
    "#     def mean_pooling(self, token_embeddings, mask):\n",
    "#         # Zero out padded tokens\n",
    "#         token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
    "#         # Mean pooling\n",
    "#         sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
    "#         return sentence_embeddings\n",
    "\n",
    "#     def embed_documents(self, texts):\n",
    "#         return [self._embed(text) for text in texts]\n",
    "\n",
    "#     def embed_query(self, text):\n",
    "#         return self._embed(text)\n",
    "\n",
    "#     def _embed(self, text):\n",
    "#         inputs = self.tokenizer(\n",
    "#             text, \n",
    "#             return_tensors=\"pt\", \n",
    "#             truncation=True, \n",
    "#             padding=True\n",
    "#         ).to(self.device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = self.model(**inputs)\n",
    "\n",
    "#         embedding = self.mean_pooling(outputs[0], inputs[\"attention_mask\"])\n",
    "#         return embedding[0].cpu().numpy().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a5f2e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:11: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
      "  from scipy.sparse import csr_matrix, issparse\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "class MiniLM(Embeddings):\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [self._embed(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self._embed(text)\n",
    "\n",
    "    def _embed(self, text):\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**inputs)\n",
    "\n",
    "        embedding = self.mean_pooling(model_output, inputs[\"attention_mask\"])\n",
    "        return embedding[0].cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36eab200",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_fn = MiniLM(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    device=\"cuda\"  # or \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55c41ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore, RetrievalMode\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Local Qdrant\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "# Create collection\n",
    "# client.create_collection(\n",
    "#     collection_name=\"wikipedia\",\n",
    "#     vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    "# )\n",
    "\n",
    "# Create embedding object\n",
    "# embedding_fn = ContrieverEmbeddings(device=\"cuda\")  # or \"cuda\"\n",
    "\n",
    "# Initialize vector store\n",
    "qdrant = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"wikipedia\",\n",
    "    embedding=embedding_fn,\n",
    "    retrieval_mode=RetrievalMode.DENSE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e3d9069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.schema import Document\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# import json\n",
    "# import os\n",
    "\n",
    "# documents = []\n",
    "# path = r'D:\\Life\\Academic\\Skripsi\\Code\\data\\wikipedia2corpus\\data\\enwiki-cleaned'\n",
    "# last_processed_file_path = r'D:\\Life\\Academic\\Skripsi\\Code\\data\\wikipedia2corpus\\last_processed.txt'\n",
    "\n",
    "# splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=512,     # max characters per chunk\n",
    "#     chunk_overlap=128    # repeated text between chunks for context\n",
    "# )\n",
    "\n",
    "# # Read last processed filename from txt\n",
    "# if os.path.exists(last_processed_file_path):\n",
    "#     with open(last_processed_file_path, \"r\") as f:\n",
    "#         last_processed_filename = f.read().strip()\n",
    "# else:\n",
    "#     last_processed_filename = \"\"\n",
    "\n",
    "# skip = True if last_processed_filename else False\n",
    "# files_processed = 0\n",
    "# max_files = 1\n",
    "# last_filename_this_run = None\n",
    "\n",
    "# if not os.path.exists(path):\n",
    "#     print(f\"Directory does not exist: {path}\")\n",
    "# else:\n",
    "#     try:\n",
    "#         for filename in sorted(os.listdir(path)):\n",
    "#             if skip:\n",
    "#                 if filename == last_processed_filename:\n",
    "#                     skip = False\n",
    "#                 continue  # Skip until after the last processed file\n",
    "\n",
    "#             file_path = os.path.join(path, filename)\n",
    "#             print(f\"Processing file: {filename}\")\n",
    "\n",
    "#             with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#                 for line in f:\n",
    "#                     data = json.loads(line)\n",
    "                    \n",
    "#                     # Split the text into chunks\n",
    "#                     chunks = splitter.split_text(data[\"text\"])\n",
    "                    \n",
    "#                     # Each chunk becomes a separate document with same metadata\n",
    "#                     for idx, chunk in enumerate(chunks):\n",
    "#                         documents.append(Document(\n",
    "#                             page_content=chunk,\n",
    "#                             metadata={\n",
    "#                                 \"title\": data[\"title\"],\n",
    "#                                 \"chunk_id\": idx  # optional for debugging\n",
    "#                             }\n",
    "#                         ))\n",
    "\n",
    "#             files_processed += 1\n",
    "#             last_filename_this_run = filename\n",
    "\n",
    "#             if files_processed >= max_files:\n",
    "#                 break\n",
    "\n",
    "#         if documents:\n",
    "#             qdrant.add_documents(documents=documents)\n",
    "#             print(f\"Processed {len(documents)} chunks successfully\")\n",
    "\n",
    "#             # Update last processed filename\n",
    "#             if last_filename_this_run:\n",
    "#                 with open(last_processed_file_path, \"w\") as f:\n",
    "#                     f.write(last_filename_this_run)\n",
    "#                 print(f\"Updated last processed filename to: {last_filename_this_run}\")\n",
    "#         else:\n",
    "#             print(\"No new files to process.\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89b3d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.schema import Document\n",
    "# import json\n",
    "# import os\n",
    "\n",
    "# documents = []\n",
    "# path = r'D:\\Life\\Academic\\Skripsi\\Code\\wikipedia2corpus\\data\\enwiki-cleaned'\n",
    "# last_processed_file_path = r'D:\\Life\\Academic\\Skripsi\\Code\\wikipedia2corpus\\last_processed.txt'\n",
    "\n",
    "# # Read last processed filename from txt\n",
    "# if os.path.exists(last_processed_file_path):\n",
    "#     with open(last_processed_file_path, \"r\") as f:\n",
    "#         last_processed_filename = f.read().strip()\n",
    "# else:\n",
    "#     last_processed_filename = \"\"\n",
    "\n",
    "# skip = True if last_processed_filename else False\n",
    "# files_processed = 0\n",
    "# max_files = 3\n",
    "# last_filename_this_run = None\n",
    "\n",
    "# if not os.path.exists(path):\n",
    "#     print(f\"Directory does not exist: {path}\")\n",
    "# else:\n",
    "#     try:\n",
    "#         for filename in sorted(os.listdir(path)):\n",
    "#             if skip:\n",
    "#                 if filename == last_processed_filename:\n",
    "#                     skip = False\n",
    "#                 continue  # Skip this file (including the last processed one)\n",
    "#             file_path = os.path.join(path, filename)\n",
    "#             print(f\"Processing file: {filename}\")\n",
    "#             with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#                 for line in f:\n",
    "#                     data = json.loads(line)\n",
    "#                     documents.append(Document(\n",
    "#                         page_content=data[\"text\"], \n",
    "#                         metadata={\"title\": data[\"title\"]}\n",
    "#                     ))\n",
    "#             files_processed += 1\n",
    "#             last_filename_this_run = filename\n",
    "#             if files_processed >= max_files:\n",
    "#                 break\n",
    "#         if documents:\n",
    "#             qdrant.add_documents(documents=documents)\n",
    "#             print(f\"Processed {len(documents)} documents successfully\")\n",
    "#             # Update last processed filename\n",
    "#             if last_filename_this_run:\n",
    "#                 with open(last_processed_file_path, \"w\") as f:\n",
    "#                     f.write(last_filename_this_run)\n",
    "#                 print(f\"Updated last processed filename to: {last_filename_this_run}\")\n",
    "#         else:\n",
    "#             print(\"No new files to process.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9ab4fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Create a prompt template that includes context\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question. If you don't know the answer based on the context, just say you don't know.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=qdrant.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a438c1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Nga taonga refer to traditional instruments from various cultures including Māori in New Zealand, Fula people in West Africa, Gabonese culture, and so on. Each culture has its unique musical traditions with their respective traditional instruments. The context provided does not provide specific information about what nga taonga sound like or are called since it is broadened to a larger array of music genres such as ngoma from Tanzania, Taarab in Zanzibar, Kwaya choir music from Tanzania, and Gumbe folk styles from various countries including the sounds produced by traditional instruments used in these music forms. These musical traditions play an important role in conveying stories, religious customs and daily life routines which hold cultural significance for their respective communities.\n"
     ]
    }
   ],
   "source": [
    "query = \"whats nga taonga sound?\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(f\"{result['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a76f6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Traditional Māori instruments are taonga pūoro. They fulfilled various roles including storytelling, religious traditions and also daily functions such as the beginning of a new day. Taonga pūoro fall into two areas, melodic instruments such as the flute and rhythmic instruments such as poi \"balls of dried flax on string that are swung and tapped\".' metadata={'_id': 250294, '_collection_name': 'wikipedia'}\n",
      "page_content='Ngoma \"(Bantu, meaning dance, drum and event)\" is a traditional dance music that has been the most widespread music in Tanzania. Dansi is urban jazz or band music. Taarab is sung Kiswahili poetry accompanied by a band, typically string, in which audience is often, but not always, encouraged to dance and clap. Kwaya is choir music originally limited to church during colonization, but now an secular part of education, social, and political events.' metadata={'_id': 103812, '_collection_name': 'wikipedia'}\n",
      "page_content='The Fula have a rich musical culture and play a variety of traditional instruments including drums, \"hoddu\" (a plucked skin-covered lute similar to a banjo), and \"riti\" or \"riiti\" (a one-string bowed instrument similar to a violin), in addition to vocal music. The well-known Senegalese Fula musician Baaba Maal sings in Pulaar on his recordings. \"Zaghareet\" or ululation is a popular form of vocal music formed by rapidly moving the tongue sideways and making a sharp, high sound.' metadata={'_id': 1143283, '_collection_name': 'wikipedia'}\n",
      "page_content='It has an array of folk styles. Imported rock and hip hop from the US and UK are in Gabon, as are rumba, makossa and soukous. Some folk instruments include the obala, the ngombi, the balafon and drums .' metadata={'_id': 397772, '_collection_name': 'wikipedia'}\n",
      "page_content='The word \"gumbe\" is sometimes used generically, to refer to any music of the country, although it most specifically refers to a unique style that fuses about ten of the country's folk music traditions. Tina and tinga are other popular genres, while extent folk traditions include ceremonial music used in funerals, initiations, and other rituals, as well as Balanta brosca and kussundé, Mandinga djambadon, and the kundere sound of the Bissagos Islands.' metadata={'_id': 589632, '_collection_name': 'wikipedia'}\n"
     ]
    }
   ],
   "source": [
    "found_docs = qdrant.similarity_search(query, k=5)\n",
    "for doc in found_docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f735c068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Nga taonga is a Māori term that means \"treasures.\" The phrase \"Nga taonga sound\" refers to the diverse collection of sounds, music, and audio content produced by humans or natural phenomena. \\n\\nIn this context, \"Nga taonga sound\" could refer to various aspects such as:\\n\\n1. Music: Any form of auditory art that is appreciated for its beauty and expressive power can be considered a Nga taonga sound. It includes traditional Maori chants, modern pop songs, classical compositions, or any other kind of music.\\n\\n2. Nature Sounds: The natural sounds present on Earth are another type of Nga taonga sound. These include things like rain falling, wind blowing through trees, waves crashing against the shoreline, bird calls, insect buzzing and so on. They create a calming effect for many people who listen to them. \\n\\n3. Cultural Sounds: It could also refer to sounds from different cultures or ethnic groups which might be important symbols of those groups\\' identity such as traditional stories narrated with voice narration by the storyteller, folk songs passed down through generations etc., they hold great value and should be preserved for future generations to come. \\n\\n4. Audio Production: This refers to any recording done to produce something that can be listened to or perceived directly through hearing it alone; like voiceovers in TV commercials, dialogues during a podcast episode, music being recorded using different instruments etc., which helps create auditory aesthetics that people might enjoy consuming over time period of their choice while relaxing at home or on the go.\\n\\n5. Electronic Sounds: Nga taonga sound can also refer to synthesized audio elements like those created by digital equipment such as synthesizers and samplers producing new sounds never heard before creating innovative musical structures for future generations’ entertainment, it\\'s an exploration of human imagination in terms of music technology advancements.\\n\\nOverall, \"Nga Taonga Sound\" is a broad term that encompasses various forms of auditory experiences from different corners of our planet; each with their unique identity and value waiting to be appreciated by listeners.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c78fdc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
