{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842ededb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_ollama import OllamaLLM\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# llm = OllamaLLM(model='deepseekmini')\n",
    "llm = ChatOllama(model='dsq4km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a5f2e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:11: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 2.3.1)\n",
      "  from scipy.sparse import csr_matrix, issparse\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "class MiniLM(Embeddings):\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [self._embed(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self._embed(text)\n",
    "\n",
    "    def _embed(self, text):\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**inputs)\n",
    "\n",
    "        embedding = self.mean_pooling(model_output, inputs[\"attention_mask\"])\n",
    "        return embedding[0].cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36eab200",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_fn = MiniLM(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    device=\"cuda\"  # or \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55c41ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore, RetrievalMode\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "qdrant = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"wikipedia\",\n",
    "    embedding=embedding_fn,\n",
    "    retrieval_mode=RetrievalMode.DENSE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1d79a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "class RetrievalResponse(BaseModel):\n",
    "    response: str = Field(..., title=\"Retrieval Necessity Judgment\", \n",
    "                         description=\"Whether retrieval is necessary for the query. Answer only 'Yes' or 'No'.\")\n",
    "\n",
    "retrieval_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"\"\"Determine if the following user query requires factual, real-world knowledge to answer. This includes questions about events, people, places, companies, historical facts, scientific concepts, or recent news.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Answer only 'Yes' or 'No'. Do not output anything else.\"\"\"\n",
    ")\n",
    "\n",
    "class RelevanceResponse(BaseModel):\n",
    "    response: str = Field(..., title=\"Relevance Judgment\", \n",
    "                         description=\"Whether the retrieved context is relevant to the query. Answer only 'Relevant' or 'Irrelevant'.\")\n",
    "\n",
    "relevance_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\"],\n",
    "    template=\"Given the query '{query}' and the retrieved context '{context}', determine if the context is relevant to the query and provides useful information to complete the task. Only answer 'Relevant' or 'Irrelevant'. Do not output anything else.\"\n",
    ")\n",
    "\n",
    "class GenerationResponse(BaseModel):\n",
    "    response: str = Field(..., title=\"Generated Response\", \n",
    "                         description=\"The response generated based on the query and context.\")\n",
    "\n",
    "generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"context\"],\n",
    "    template=\"\"\"You are a helpful AI assistant. Generate a response to the query based on the provided context. If the context is relevant, use it to answer the query accurately. If the context is irrelevant, rely on your own knowledge but indicate any uncertainties or lack of information. Be concise and informative. If you are presented a multiple choice, only answer with the letter, do not include anything else.\n",
    "\n",
    "Query: {query}\n",
    "Context: {context}\n",
    "\n",
    "Response:\"\"\"\n",
    ")\n",
    "\n",
    "class SupportResponse(BaseModel):\n",
    "    response: str = Field(..., title=\"Support Judgment\", \n",
    "                         description=\"Whether the response is supported by the context. Answer only 'Fully supported', 'Partially supported', or 'No support'.\")\n",
    "\n",
    "support_prompt = PromptTemplate(\n",
    "    input_variables=[\"response\", \"context\"],\n",
    "    template=\"\"\"Given the response '{response}' to a query, and the information provided '{context}' for response generation, determine if the response is supported by the information. \n",
    "\n",
    "Use the following entailment scale to generate a score:\n",
    "- Fully supported: All information in output is supported by the evidence, or extraction from the evidence. This is only applicable when the output and part of the evidence are almost identical.\n",
    "- Partially supported: The output is supported by the evidence to some extent, but there is major information in the output that is not discussed in the evidence.\n",
    "- No support: The output completely ignores evidence, is unrelated to the evidence, or contradicts the evidence.\n",
    "\n",
    "Make sure to not use any external information/knowledge to judge whether the output is true or not. Only check whether the output is supported by the evidence.\n",
    "\n",
    "Only answer 'Fully supported', 'Partially supported', or 'No support'. Do not output anything else.\"\"\"\n",
    ")\n",
    "\n",
    "class UtilityResponse(BaseModel):\n",
    "    response: int = Field(..., title=\"Utility Score\", \n",
    "                         description=\"The utility score of the response from 1 to 5, where 5 is highest utility.\")\n",
    "\n",
    "utility_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"response\"],\n",
    "    template=\"\"\"Given the query '{query}' and the response '{response}', rate the perceived utility score of the response from 1 (lowest) to 5 (highest). \n",
    "\n",
    "The detailed criterion is as follows:\n",
    "5: The response provides a complete, highly detailed, and informative response to the query, fully satisfying the information needs.\n",
    "4: The response mostly fulfills the need in the query, while there can be some minor improvements such as discussing more detailed information, having better structure of the response, or improving coherence.\n",
    "3: The response is acceptable, but some major additions or improvements are needed to satisfy users' needs.\n",
    "2: The response still addresses the main request, but it is not complete or not relevant to the query.\n",
    "1: The response is barely on-topic or completely irrelevant.\n",
    "\n",
    "Only answer a single number between 1 and 5. Do not output anything else.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "74d769cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = retrieval_prompt | llm.with_structured_output(RetrievalResponse)\n",
    "relevance_chain = relevance_prompt | llm.with_structured_output(RelevanceResponse)\n",
    "generation_chain = generation_prompt | llm.with_structured_output(GenerationResponse)\n",
    "support_chain = support_prompt | llm.with_structured_output(SupportResponse)\n",
    "utility_chain = utility_prompt | llm.with_structured_output(UtilityResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e78a4c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_response(response_tuple):\n",
    "    response, support, utility = response_tuple\n",
    "    support_score = 3 if support == 'fully supported' else (2 if support == 'partially supported' else 1)\n",
    "    return support_score * 10 + utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9ab4fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_rag(query, vectorstore, top_k=5):\n",
    "    log = []  # collect logs here\n",
    "    \n",
    "    def log_msg(message):\n",
    "        log.append(message)\n",
    "    \n",
    "    # Step 1: Determine if retrieval is necessary\n",
    "    input_data = {\"query\": query}\n",
    "    retrieval_decision = retrieval_chain.invoke(input_data).response.strip().lower()\n",
    "    log_msg(f\"Retrieval decision: {retrieval_decision}\")\n",
    "    \n",
    "    if \"yes\" in retrieval_decision:\n",
    "        # Step 2: Retrieve relevant documents\n",
    "        docs = vectorstore.similarity_search(query, k=top_k)\n",
    "        contexts = [doc.page_content for doc in docs]\n",
    "        \n",
    "        # Step 3: Evaluate relevance of retrieved documents\n",
    "        relevant_contexts = []\n",
    "        for i, context in enumerate(contexts):\n",
    "            input_data = {\"query\": query, \"context\": context}\n",
    "            relevance = relevance_chain.invoke(input_data).response.strip().lower()\n",
    "            log_msg(f\"Document {i+1} relevance: {relevance}\")\n",
    "            if relevance == 'relevant':\n",
    "                relevant_contexts.append(context)\n",
    "        \n",
    "        log_msg(f\"No of relevant context: {len(relevant_contexts)}\")\n",
    "        \n",
    "        # If no relevant contexts found, generate without retrieval\n",
    "        if not relevant_contexts:\n",
    "            log_msg(\"No relevant contexts found. Generating without retrieval...\")\n",
    "            input_data = {\"query\": query, \"context\": \"No relevant context found. Answer the query anyway.\"}\n",
    "            return generation_chain.invoke(input_data).response, \"\\n\".join(log)\n",
    "        \n",
    "        # Step 4: Generate response using relevant contexts\n",
    "        responses = []\n",
    "        for i, context in enumerate(relevant_contexts):\n",
    "            input_data = {\"query\": query, \"context\": context}\n",
    "            response = generation_chain.invoke(input_data).response\n",
    "            \n",
    "            # Step 5: Assess support\n",
    "            input_data = {\"response\": response, \"context\": context}\n",
    "            support = support_chain.invoke(input_data).response.strip().lower()\n",
    "            log_msg(f\"Support assessment for response {i+1}: {support}\")\n",
    "            \n",
    "            # Step 6: Evaluate utility\n",
    "            input_data = {\"query\": query, \"response\": response}\n",
    "            utility = int(utility_chain.invoke(input_data).response)\n",
    "            log_msg(f\"Utility score for response {i+1}: {utility}\")\n",
    "            \n",
    "            responses.append((response, support, utility))\n",
    "        \n",
    "        # Select the best response based on support and utility\n",
    "        best_response = max(responses, key=score_response)\n",
    "        response, support, utility = best_response\n",
    "\n",
    "        log_msg(f\"Best response support: {support}, utility: {utility}\")\n",
    "\n",
    "        if len(context) > 100:\n",
    "            log_msg(f\"Used context: {context[:100]}...\")\n",
    "        else:\n",
    "            log_msg(f\"Used context: {context}\")\n",
    "        \n",
    "        return response, \"\\n\".join(log)\n",
    "    else:\n",
    "        # Generate without retrieval\n",
    "        log_msg(\"Generating without retrieval...\")\n",
    "        input_data = {\"query\": query, \"context\": \"No retrieval necessary as the query can be answered with general knowledge.\"}\n",
    "        return generation_chain.invoke(input_data).response, \"\\n\".join(log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60405004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Create a prompt template that includes context\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question. If you don't know the answer based on the context, or context is irrelevant, say so and answer with general knowledge.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=qdrant.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a438c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = '''\n",
    "# \"Which naturalised American electrical/mechanical engineer and inventor (1856-1943) has given his name to the SI unit of magnetic flux density?\"\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beb19d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dokumen yang ditemukan\n",
    "# found_docs = qdrant.similarity_search(query, k=5)\n",
    "# for doc in found_docs:\n",
    "#     print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "003b088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response, log = self_rag(query, qdrant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf263475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Jawaban dengan self-rag\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f735c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Jawaban tanpa apa-apa\n",
    "# llm.invoke(query).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc7ad6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Jawaban dengan rag\n",
    "# result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "# print(f\"{result['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3268c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path.cwd().parent / \"data\"\n",
    "\n",
    "triviaqa_dir = data_dir / \"triviaqa\" / \"triviaqa-unfiltered\" / \"test.json\"\n",
    "popqa_dir = data_dir / \"popqa\" / \"test.json\"\n",
    "arc_dir = data_dir / \"arc\" / \"arc.json\"\n",
    "result_dir = data_dir / \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8452437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def evaluate(benchmark, mode=\"selfrag\", name=\"default\", limit=None, shuffle=False):\n",
    "    \"\"\"\n",
    "    Evaluate benchmark questions with different modes.\n",
    "    \n",
    "    mode options:\n",
    "        - \"selfrag\": uses self_rag(query, qdrant)\n",
    "        - \"norag\":   uses llm.invoke(query)\n",
    "        - \"rag\":     uses qa_chain.invoke({\"query\": query})\n",
    "    \"\"\"\n",
    "\n",
    "    response_file = result_dir / (name + \"-r.jsonl\")\n",
    "    log_file = result_dir / (name + \"-l.jsonl\")\n",
    "    # load benchmark data\n",
    "    with open(benchmark, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(data)\n",
    "\n",
    "    if limit > len(data) or limit is None or limit < 0:\n",
    "        limit = len(data)\n",
    "    \n",
    "    with open(response_file, 'w', encoding='utf-8') as rf, open(log_file, 'w', encoding='utf-8') as lf:\n",
    "        for idx, item in enumerate(data[:limit]):\n",
    "            query = item['Question']\n",
    "            answer = item['Answer']\n",
    "\n",
    "            # --- Run according to mode ---\n",
    "            if mode == \"selfrag\":\n",
    "                response, log = self_rag(query, qdrant)\n",
    "\n",
    "            elif mode == \"norag\":\n",
    "                response = llm.invoke(query).content\n",
    "                log = None  # no log\n",
    "\n",
    "            elif mode == \"rag\":\n",
    "                result = qa_chain.invoke({\"query\": query})\n",
    "                response = result['result']\n",
    "                docs = result.get('source_documents', [])\n",
    "                log = [doc.page_content for doc in docs]  # store docs content\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "            # --- Check correctness ---\n",
    "            responselower = response.lower()\n",
    "            anslower = [ans.lower() for ans in answer]\n",
    "            correct = 1 if any(ans in responselower for ans in anslower) else 0\n",
    "\n",
    "            # --- Write response JSONL ---\n",
    "            json.dump({\n",
    "                'id': idx,\n",
    "                'query': query,\n",
    "                'response': response,\n",
    "                'answer': answer,\n",
    "                'correct': correct\n",
    "            }, rf, ensure_ascii=False)\n",
    "            rf.write('\\n')\n",
    "\n",
    "            # --- Write log JSONL ---\n",
    "            json.dump({\n",
    "                'id': idx,\n",
    "                'query': query,\n",
    "                'log': log\n",
    "            }, lf, ensure_ascii=False)\n",
    "            lf.write('\\n')\n",
    "\n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"Processed {idx + 1} / {limit} questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba2e8c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(response_file):\n",
    "    with open(response_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    total = len(lines)\n",
    "    correct = sum(1 for line in lines if json.loads(line)['correct'] == 1)\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b75354d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(log_file, response_file):\n",
    "    # Track IDs by retrieval decision\n",
    "    no_retrieval_ids = set()\n",
    "    yes_retrieval_ids = set()\n",
    "    \n",
    "    with open(log_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            log_entry = json.loads(line)\n",
    "            if \"Retrieval decision: no\" in log_entry['log']:\n",
    "                no_retrieval_ids.add(log_entry['id'])\n",
    "            elif \"Retrieval decision: yes\" in log_entry['log']:\n",
    "                yes_retrieval_ids.add(log_entry['id'])\n",
    "    \n",
    "    # Initialize counters\n",
    "    no_correct = no_wrong = 0\n",
    "    yes_correct = yes_wrong = 0\n",
    "    total_correct = total_wrong = 0\n",
    "    \n",
    "    with open(response_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            response_entry = json.loads(line)\n",
    "            rid = response_entry['id']\n",
    "            correct = response_entry['correct'] == 1\n",
    "            \n",
    "            # Update totals\n",
    "            if correct:\n",
    "                total_correct += 1\n",
    "            else:\n",
    "                total_wrong += 1\n",
    "            \n",
    "            # Update retrieval-specific counts\n",
    "            if rid in no_retrieval_ids:\n",
    "                if correct:\n",
    "                    no_correct += 1\n",
    "                else:\n",
    "                    no_wrong += 1\n",
    "            elif rid in yes_retrieval_ids:\n",
    "                if correct:\n",
    "                    yes_correct += 1\n",
    "                else:\n",
    "                    yes_wrong += 1\n",
    "    \n",
    "    total_answers = total_correct + total_wrong\n",
    "    \n",
    "    return {\n",
    "        \"total_answers\": total_answers,\n",
    "        \"total_correct\": total_correct,\n",
    "        \"total_wrong\": total_wrong,\n",
    "        \"no_retrieval_correct\": no_correct,\n",
    "        \"no_retrieval_wrong\": no_wrong,\n",
    "        \"yes_retrieval_correct\": yes_correct,\n",
    "        \"yes_retrieval_wrong\": yes_wrong,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fccdfeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_inconsistent_retrieval(log_file):\n",
    "    inconsistent_count = 0\n",
    "    with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line in f:\n",
    "            log_entry = json.loads(line)\n",
    "            if \"Retrieval decision:\" in log_entry['log']:\n",
    "                if (\"Retrieval decision: no\" not in log_entry['log'] and\n",
    "                    \"Retrieval decision: yes\" not in log_entry['log']):\n",
    "                    inconsistent_count += 1\n",
    "                    # print what comes after \"Retrieval decision:\"\n",
    "                    print(log_entry['log'].split(\"Retrieval\")[1].strip())\n",
    "    return inconsistent_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c1aa9949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_analysis(name):\n",
    "    response_file = result_dir / f\"{name}-r.jsonl\"\n",
    "    log_file = result_dir / f\"{name}-l.jsonl\"\n",
    "\n",
    "    accuracy = compute_accuracy(response_file)\n",
    "    try:\n",
    "        analysis = analyze(log_file, response_file)\n",
    "        inconsistent_count = count_inconsistent_retrieval(log_file)\n",
    "    except Exception:\n",
    "        analysis = {\n",
    "            \"total_answers\": 0,\n",
    "            \"total_correct\": 0,\n",
    "            \"total_wrong\": 0,\n",
    "            \"no_retrieval_correct\": 0,\n",
    "            \"no_retrieval_wrong\": 0,\n",
    "            \"yes_retrieval_correct\": 0,\n",
    "            \"yes_retrieval_wrong\": 0,\n",
    "        }\n",
    "        inconsistent_count = 0\n",
    "\n",
    "    print(f\"Overall Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Total Answers: {analysis['total_answers']}\")\n",
    "    print(f\"Total Correct: {analysis['total_correct']}\")\n",
    "    print(f\"Total Wrong: {analysis['total_wrong']}\")\n",
    "    print(f\"No Retrieval - Correct: {analysis['no_retrieval_correct']}, Wrong: {analysis['no_retrieval_wrong']}\")\n",
    "    print(f\"Yes Retrieval - Correct: {analysis['yes_retrieval_correct']}, Wrong: {analysis['yes_retrieval_wrong']}\")\n",
    "    print(f\"Inconsistent Retrieval Decisions: {inconsistent_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eeae1c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "80cc4451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 / 100 questions.\n",
      "Processed 20 / 100 questions.\n",
      "Processed 30 / 100 questions.\n",
      "Processed 40 / 100 questions.\n",
      "Processed 50 / 100 questions.\n",
      "Processed 60 / 100 questions.\n",
      "Processed 70 / 100 questions.\n",
      "Processed 80 / 100 questions.\n",
      "Processed 90 / 100 questions.\n",
      "Processed 100 / 100 questions.\n"
     ]
    }
   ],
   "source": [
    "evaluate(triviaqa_dir, mode=\"selfrag\", name=name, limit=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9e68a01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision: the question requires factual real-world knowledge to answer.\n",
      "Generating without retrieval...\n",
      "decision: partially correct information\n",
      "Generating without retrieval...\n",
      "decision: partially correct answer\n",
      "Generating without retrieval...\n",
      "Overall Accuracy: 47.00%\n",
      "Total Answers: 100\n",
      "Total Correct: 47\n",
      "Total Wrong: 53\n",
      "No Retrieval - Correct: 16, Wrong: 21\n",
      "Yes Retrieval - Correct: 29, Wrong: 31\n",
      "Inconsistent Retrieval Decisions: 3\n"
     ]
    }
   ],
   "source": [
    "accuracy_analysis(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c22097de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 / 100 questions.\n",
      "Processed 20 / 100 questions.\n",
      "Processed 30 / 100 questions.\n",
      "Processed 40 / 100 questions.\n",
      "Processed 50 / 100 questions.\n",
      "Processed 60 / 100 questions.\n",
      "Processed 70 / 100 questions.\n",
      "Processed 80 / 100 questions.\n",
      "Processed 90 / 100 questions.\n",
      "Processed 100 / 100 questions.\n"
     ]
    }
   ],
   "source": [
    "name = 'poptest'\n",
    "evaluate(popqa_dir, mode=\"selfrag\", name=name, limit=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e17f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 18.00%\n",
      "Total Answers: 100\n",
      "Total Correct: 18\n",
      "Total Wrong: 82\n",
      "No Retrieval - Correct: 10, Wrong: 34\n",
      "Yes Retrieval - Correct: 8, Wrong: 48\n",
      "Inconsistent Retrieval Decisions: 0\n"
     ]
    }
   ],
   "source": [
    "accuracy_analysis(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964dd8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 / 100 questions.\n",
      "Processed 20 / 100 questions.\n",
      "Processed 30 / 100 questions.\n",
      "Processed 40 / 100 questions.\n",
      "Processed 50 / 100 questions.\n",
      "Processed 60 / 100 questions.\n",
      "Processed 70 / 100 questions.\n",
      "Processed 80 / 100 questions.\n",
      "Processed 90 / 100 questions.\n",
      "Processed 100 / 100 questions.\n",
      "true\n",
      "Generating without retrieval...\n",
      "Overall Accuracy: 70.00%\n",
      "Total Answers: 100\n",
      "Total Correct: 70\n",
      "Total Wrong: 30\n",
      "No Retrieval - Correct: 20, Wrong: 12\n",
      "Yes Retrieval - Correct: 49, Wrong: 18\n",
      "Inconsistent Retrieval Decisions: 1\n"
     ]
    }
   ],
   "source": [
    "name = 'arctest'\n",
    "evaluate(arc_dir, mode=\"selfrag\", name=name, limit=100, shuffle=True)\n",
    "accuracy_analysis(name)\n",
    "\n",
    "# name = 'arctest2'\n",
    "# evaluate(arc_dir, mode=\"selfrag\", name=name, limit=100, shuffle=True)\n",
    "# accuracy_analysis(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "652a40b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 / 100 questions.\n",
      "Processed 20 / 100 questions.\n",
      "Processed 30 / 100 questions.\n",
      "Processed 40 / 100 questions.\n",
      "Processed 50 / 100 questions.\n",
      "Processed 60 / 100 questions.\n",
      "Processed 70 / 100 questions.\n",
      "Processed 80 / 100 questions.\n",
      "Processed 90 / 100 questions.\n",
      "Processed 100 / 100 questions.\n",
      "Overall Accuracy: 58.00%\n",
      "Total Answers: 0\n",
      "Total Correct: 0\n",
      "Total Wrong: 0\n",
      "No Retrieval - Correct: 0, Wrong: 0\n",
      "Yes Retrieval - Correct: 0, Wrong: 0\n",
      "Inconsistent Retrieval Decisions: 0\n",
      "Processed 10 / 100 questions.\n",
      "Processed 20 / 100 questions.\n",
      "Processed 30 / 100 questions.\n",
      "Processed 40 / 100 questions.\n",
      "Processed 50 / 100 questions.\n",
      "Processed 60 / 100 questions.\n",
      "Processed 70 / 100 questions.\n",
      "Processed 80 / 100 questions.\n",
      "Processed 90 / 100 questions.\n",
      "Processed 100 / 100 questions.\n",
      "Overall Accuracy: 23.00%\n",
      "Total Answers: 0\n",
      "Total Correct: 0\n",
      "Total Wrong: 0\n",
      "No Retrieval - Correct: 0, Wrong: 0\n",
      "Yes Retrieval - Correct: 0, Wrong: 0\n",
      "Inconsistent Retrieval Decisions: 0\n",
      "Processed 10 / 100 questions.\n",
      "Processed 20 / 100 questions.\n",
      "Processed 30 / 100 questions.\n",
      "Processed 40 / 100 questions.\n",
      "Processed 50 / 100 questions.\n",
      "Processed 60 / 100 questions.\n",
      "Processed 70 / 100 questions.\n",
      "Processed 80 / 100 questions.\n",
      "Processed 90 / 100 questions.\n",
      "Processed 100 / 100 questions.\n",
      "Overall Accuracy: 95.00%\n",
      "Total Answers: 0\n",
      "Total Correct: 0\n",
      "Total Wrong: 0\n",
      "No Retrieval - Correct: 0, Wrong: 0\n",
      "Yes Retrieval - Correct: 0, Wrong: 0\n",
      "Inconsistent Retrieval Decisions: 0\n"
     ]
    }
   ],
   "source": [
    "# normal mode\n",
    "name = 'defaulttriviaqa'\n",
    "evaluate(triviaqa_dir, mode=\"norag\", name=name, limit=100, shuffle=True)\n",
    "accuracy_analysis(name)\n",
    "\n",
    "name = 'defaultpopqa'\n",
    "evaluate(popqa_dir, mode=\"norag\", name=name, limit=100, shuffle=True)\n",
    "accuracy_analysis(name)\n",
    "\n",
    "name = 'defaultarc'\n",
    "evaluate(arc_dir, mode=\"norag\", name=name, limit=100, shuffle=True)\n",
    "accuracy_analysis(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
